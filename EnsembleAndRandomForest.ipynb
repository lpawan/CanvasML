{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat do you understand by Ensemble technique? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble is a collection of lot of trees and then we make predicitoin based on outcomes of those trees. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. It can be divided in two parts: Bagging and Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tExplain the idea behind ensemble techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. Rather than just relying on one Decision Tree and hoping we made the right decision at each split, Ensemble Methods allow us to take a sample of Decision Trees into account, calculate which features to use or questions to ask at each split, and make a final predictor based on the aggregated results of the sampled Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat is Bootstrapping? How is sampling done in bootstrapping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. We can solve both classification and regression problem using bootstrapping algorithim. Bootstrapping is a type of resampling where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample. We then replace those numbers into the sample and draw three numbers again. Repeat the process of drawing x numbers B times. In classificcation the final prediction will be the one with maximum count. For regression problem the final prediction is the average of each outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat is bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a simple and very powerful ensemble method. An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model. Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging. The only parameters when bagging decision trees is the number of samples and hence the number of trees to include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow prediction is made in Bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve both classification and regression problem using bootstrapping algorithim.  In classificcation the final prediction will be the one with maximum count. For regression problem the final prediction is the average of each outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tHow Ensemble technique solves the high variance issue with Decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is pasting? How is it different from bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting is an ensemble technique similar to bagging with the only difference being that there is no replacement done while sampling the training dataset. This causes less diversity in the sampled datasets and data ends up being correlated. That's why bagging is more preffered than pasting in real scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8)\tWhat is Out Of Bag evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging, when different samples are collected, no sample contains all the data but a fraction of the original dataset.\n",
    "There might be some data which are never sampled at all. The remaining data which are not sampled are called out of bag instances. Since the model never trains over these data, they can be used for evaluating the accuracy of the model by using these data for predicition. We do not need validation set or cross validation and can use out of bag instances for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)\tHow does a Random Forest model works?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to bagging different samples are collected from the training data set and then for each sample we train out tree model. Once trees are formed prediction is made by random forest by aggregating the result of all the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10)\tWhat is the difference between Bagging and Random forest? Why do we use Random forest more commonly than Bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\n",
    "Predictions made using random forest is more close to the real values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11)\tWhat is feature sampling? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)\tHow prediction is made in Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is made by the random forest by aggregating the predictions of all the model. For regression model, the mean of all the predictions is the final prediction and for classification mode, the mode of all the predictions is considered the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13)\tWhen should we not use Random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It surely does a good job at classification but not as for regression problem as it does not gives precise continuous nature prediction. In case of regression, it doesn't predict beyond the range in the training data, and that they may over fit data sets that are particularly noisy.\n",
    "\n",
    "Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)\tWhat is Stacking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is a type of ensemble technique which combines the predictions of two or more models, also called base models, and use the combination as the input for a new model (meta-model) i.e. a new model is trained on the predictions of the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "15)\tExplain the working behind Stacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Split the dataset in training and hold out data set.\n",
    "2. Split training into training and testing data sets.\n",
    "3. Train all model using training data set.\n",
    "4. After training the model, get the predictions.\n",
    "5. Stack all the predictions together and it will be used as a input feature for our meta_model.\n",
    "6. again get all the predictions of the base models on test data sets\n",
    "7. Stack all the predictions together and it will be used as a input feature for our meta_model.\n",
    "8. Use the stacked data from step 5 as the input feature for meta_model and validation set y2 as the target variable and train the model on these data.\n",
    "9. Once, the training is done check the accuracy of meta_model by using data from step 7 for prediction and y1_test for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tExplain the working of KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbors is a supervised machine learning algorithm often used in classification problems. It works on the simple assumption that things are always in close proximity.\n",
    "Load the model\n",
    "Calculate the distance between test data and each row of training data.\n",
    "Sort the calculated distances in ascending order based on distance values.\n",
    "Get top k rows from the sorted array.\n",
    "Get the most frequent class of these rows.\n",
    "Return the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tHow KNN predicts the output for regression and classification problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression KNN takes the average or mean of the points. For classification problem KNN classifies the new points based on the maximim number of nearest neighbours. Nearest neighbour are calculated using Euclidean distance, Hamming diatnce or Manhattan distance algorithims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are the different distances used in KNN? How are they calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different distance used in KNN are: Euclidean distance, Hamming diatnce or Manhattan distance.\n",
    "Euclidean distance is square root of sum of square of difference between the co ordinates.\n",
    "Hamming distance is somewhat similar to one hot encoding, it converts each class into integer number and difference between same class is represented as 0 and difference between different class is represented as 1. it is mostly used for categorical data.\n",
    "Manhattan distance is sum of mod of difference between x and y co-ordinates. More cost effective as compared to Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat are Lazy Learners? Why KNN is called a lazy learner?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy learners are those which tries to learns when they have to make a prediction. KNN is a lazy learner because in KNN there is nothing to learn at the time of training, only at the time of prediction when a new data point is given and value of k is given KNN tries to find the nearest neighbours and then classifies/predicts to which class it will belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow do we select the value of k? How bias and variance varies with k?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flexibility of the model decreases with the increase of ‘k’. With lower value of ‘k’ variance is high and bias is low but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing. With very low values of ‘k’ there is a chance of algorithm overfitting the data whereas with very high value of ‘k’ there is a chance of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat are advantages and disadvantages of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "1. Training phase is simple and fast.\n",
    "2. Mathematics behind the algorithim is simple and easy to understand\n",
    "3. Can be used for both regression and classification problems.\n",
    "4. Simple and easy to understand\n",
    "\n",
    "Disadvantages:\n",
    "1. Large space requirement.\n",
    "2. Time required to predict is also more because KNN is lazy learner - high time complexity\n",
    "3. High space omplexity\n",
    "4. It takes lot of time to compute the disance of test point with all other data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tDiscuss kDTree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KD Tree algorithim uses a mixture of Decision trees and KNN to calculate the nearest neighbour(s). It rearranges the whole data set into a binary tree and when a new test data is provided it traverses the tree from the root node towards the leaf node and gives us the result using a brute force approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8)\tDiscuss Ball Tree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a hierarchical data structture. We start with 2 clusters, all the data points must belong to one cluster and one point cannot be in more than one cluster. Distance of each new point is calculated from the centroid of each cluster and the distance whci is minimum that point is assigned to that cluster. Initialy ball tree clsuter formation takes lot of time but once the clsuters are formed then finding the nearest neighbous is very fast and easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
